{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49b4a288-b82f-49a3-abe6-705bd60e7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"data/raw/german_credit.csv\")\n",
    "Path(\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"splits\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a83f1ee7-1c25-4c91-b0ca-3bc887528ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:320: UserWarning: Multiple active versions of the dataset matching the name credit-g exist. Versions may be fundamentally different, returning version 1. Available versions:\n",
      "- version 1, status: active\n",
      "  url: https://www.openml.org/search?type=data&id=31\n",
      "- version 2, status: active\n",
      "  url: https://www.openml.org/search?type=data&id=44096\n",
      "\n",
      "  warn(warning_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((1000, 21), np.float64(0.3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "DATA_PATH = Path(\"data/raw/german_credit.csv\")\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "else:\n",
    "    try:\n",
    "        # try by name\n",
    "        data = fetch_openml(name=\"credit-g\", as_frame=True, parser=\"pandas\")\n",
    "    except Exception as e1:\n",
    "        # fallback: try by ID\n",
    "        data = fetch_openml(data_id=31, as_frame=True, parser=\"pandas\")\n",
    "\n",
    "    df = data.frame\n",
    "    # make binary target\n",
    "    if \"class\" in df.columns:\n",
    "        df[\"default\"] = (df[\"class\"] == \"bad\").astype(int)\n",
    "        df = df.drop(columns=[\"class\"])\n",
    "    # tidy names\n",
    "    df.columns = [c.replace(\".\", \"_\").lower() for c in df.columns]\n",
    "    # persist a clean copy\n",
    "    DATA_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(DATA_PATH, index=False)\n",
    "\n",
    "df.shape, df[\"default\"].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5fdaeb4-3f54-4b17-8d16-58bab8ed5b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1000, 21)\n",
      "target rate: 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>checking_status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>savings_status</th>\n",
       "      <th>employment</th>\n",
       "      <th>installment_commitment</th>\n",
       "      <th>personal_status</th>\n",
       "      <th>other_parties</th>\n",
       "      <th>...</th>\n",
       "      <th>property_magnitude</th>\n",
       "      <th>age</th>\n",
       "      <th>other_payment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>existing_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>num_dependents</th>\n",
       "      <th>own_telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;0</td>\n",
       "      <td>6</td>\n",
       "      <td>critical/other existing credit</td>\n",
       "      <td>radio/tv</td>\n",
       "      <td>1169</td>\n",
       "      <td>no known savings</td>\n",
       "      <td>&gt;=7</td>\n",
       "      <td>4</td>\n",
       "      <td>male single</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real estate</td>\n",
       "      <td>67</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>2</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0&lt;=X&lt;200</td>\n",
       "      <td>48</td>\n",
       "      <td>existing paid</td>\n",
       "      <td>radio/tv</td>\n",
       "      <td>5951</td>\n",
       "      <td>&lt;100</td>\n",
       "      <td>1&lt;=X&lt;4</td>\n",
       "      <td>2</td>\n",
       "      <td>female div/dep/mar</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real estate</td>\n",
       "      <td>22</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>skilled</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no checking</td>\n",
       "      <td>12</td>\n",
       "      <td>critical/other existing credit</td>\n",
       "      <td>education</td>\n",
       "      <td>2096</td>\n",
       "      <td>&lt;100</td>\n",
       "      <td>4&lt;=X&lt;7</td>\n",
       "      <td>2</td>\n",
       "      <td>male single</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>real estate</td>\n",
       "      <td>49</td>\n",
       "      <td>none</td>\n",
       "      <td>own</td>\n",
       "      <td>1</td>\n",
       "      <td>unskilled resident</td>\n",
       "      <td>2</td>\n",
       "      <td>none</td>\n",
       "      <td>yes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  checking_status  duration                  credit_history    purpose  \\\n",
       "0              <0         6  critical/other existing credit   radio/tv   \n",
       "1        0<=X<200        48                   existing paid   radio/tv   \n",
       "2     no checking        12  critical/other existing credit  education   \n",
       "\n",
       "   credit_amount    savings_status employment  installment_commitment  \\\n",
       "0           1169  no known savings        >=7                       4   \n",
       "1           5951              <100     1<=X<4                       2   \n",
       "2           2096              <100     4<=X<7                       2   \n",
       "\n",
       "      personal_status other_parties  ...  property_magnitude age  \\\n",
       "0         male single          none  ...         real estate  67   \n",
       "1  female div/dep/mar          none  ...         real estate  22   \n",
       "2         male single          none  ...         real estate  49   \n",
       "\n",
       "   other_payment_plans housing existing_credits                 job  \\\n",
       "0                 none     own                2             skilled   \n",
       "1                 none     own                1             skilled   \n",
       "2                 none     own                1  unskilled resident   \n",
       "\n",
       "  num_dependents  own_telephone foreign_worker default  \n",
       "0              1            yes            yes       0  \n",
       "1              1           none            yes       1  \n",
       "2              2           none            yes       0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'rows': 1000,\n",
       " 'cols': 21,\n",
       " 'positive_rate': 0.3,\n",
       " 'n_numeric': 7,\n",
       " 'n_categorical': 13}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.api.types import is_numeric_dtype, is_categorical_dtype, is_object_dtype, is_bool_dtype\n",
    "\n",
    "print(\"shape:\", df.shape)\n",
    "print(\"target rate:\", df[\"default\"].mean())\n",
    "\n",
    "# Identify dtypes safely\n",
    "numeric = [c for c in df.columns if c != \"default\" and is_numeric_dtype(df[c])]\n",
    "categorical = [\n",
    "    c for c in df.columns\n",
    "    if c != \"default\" and (\n",
    "        isinstance(df[c].dtype, pd.CategoricalDtype) or\n",
    "        df[c].dtype == \"object\" or\n",
    "        is_bool_dtype(df[c])\n",
    "    )\n",
    "]\n",
    "\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "cardinality = df.nunique().sort_values(ascending=False)\n",
    "\n",
    "summary = {\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"positive_rate\": float(df[\"default\"].mean()),\n",
    "    \"n_numeric\": len(numeric),\n",
    "    \"n_categorical\": len(categorical),\n",
    "}\n",
    "\n",
    "display(df.head(3))\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86e24ab-4a1f-413c-971f-6de805c57df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: reports/eda_summary.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"reports\", exist_ok=True)\n",
    "\n",
    "eda = {\n",
    "    \"rows\": int(df.shape[0]),\n",
    "    \"cols\": int(df.shape[1]),\n",
    "    \"positive_rate\": float(df[\"default\"].mean()),\n",
    "    \"numeric\": numeric,\n",
    "    \"categorical\": categorical,\n",
    "    \"top_missing\": missing.head(10).to_dict(),\n",
    "    \"top_cardinality\": cardinality.head(10).to_dict(),\n",
    "}\n",
    "with open(\"reports/eda_summary.json\", \"w\") as f:\n",
    "    json.dump(eda, f, indent=2)\n",
    "print(\"saved: reports/eda_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d446d6f1-419b-4428-92c9-73cf74da940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "os.makedirs(\"splits\", exist_ok=True)\n",
    "\n",
    "y = df[\"default\"]\n",
    "X = df.drop(columns=[\"default\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.40, stratify=y, random_state=42\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "pd.DataFrame({\"index\": X_train.index}).to_csv(\"splits/train_idx.csv\", index=False)\n",
    "pd.DataFrame({\"index\": X_valid.index}).to_csv(\"splits/valid_idx.csv\", index=False)\n",
    "pd.DataFrame({\"index\": X_test.index}).to_csv(\"splits/test_idx.csv\", index=False)\n",
    "print(\"splits saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "767baa14-f6e1-4364-a279-cd1681e9be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: reports/feature_schema.json\n"
     ]
    }
   ],
   "source": [
    "schema = {\"numeric\": numeric, \"categorical\": categorical, \"target\": \"default\"}\n",
    "with open(\"reports/feature_schema.json\", \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "print(\"saved: reports/feature_schema.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd4d2ab8-a736-46a7-84e4-b5a973cc4a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\balla\\Documents\\credit-risk-scorecard\\notebooks\n",
      "Repo exists here?  True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"Repo exists here? \", (Path.home() / \"Documents\" / \"credit-risk-scorecard\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c1ef65-c2ff-4d62-b855-2d8c9e8b4135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moved files to repo root\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "REPO = Path.home() / \"Documents\" / \"credit-risk-scorecard\"\n",
    "NB   = REPO / \"notebooks\"\n",
    "\n",
    "# sources \n",
    "src_csv   = NB / \"data\" / \"raw\" / \"german_credit.csv\"\n",
    "src_splits= NB / \"splits\"\n",
    "src_reports = NB / \"reports\"\n",
    "\n",
    "# destinations \n",
    "dst_csv   = REPO / \"data\" / \"raw\" / \"german_credit.csv\"\n",
    "dst_splits= REPO / \"splits\"\n",
    "dst_reports = REPO / \"reports\"\n",
    "\n",
    "dst_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "dst_splits.mkdir(parents=True, exist_ok=True)\n",
    "dst_reports.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# move CSV\n",
    "if src_csv.exists():\n",
    "    shutil.move(str(src_csv), str(dst_csv))\n",
    "\n",
    "# move split CSVs\n",
    "if src_splits.exists():\n",
    "    for p in src_splits.glob(\"*.csv\"):\n",
    "        shutil.move(str(p), str(dst_splits / p.name))\n",
    "\n",
    "# move report JSONs\n",
    "if src_reports.exists():\n",
    "    for p in src_reports.glob(\"*.json\"):\n",
    "        shutil.move(str(p), str(dst_reports / p.name))\n",
    "\n",
    "print(\"moved files to repo root\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b03e4ff0-ea60-400f-9877-16f6823d9ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw: [WindowsPath('C:/Users/balla/Documents/credit-risk-scorecard/data/raw/german_credit.csv')]\n",
      "splits: [WindowsPath('C:/Users/balla/Documents/credit-risk-scorecard/splits/test_idx.csv'), WindowsPath('C:/Users/balla/Documents/credit-risk-scorecard/splits/train_idx.csv'), WindowsPath('C:/Users/balla/Documents/credit-risk-scorecard/splits/valid_idx.csv')]\n",
      "reports: [WindowsPath('C:/Users/balla/Documents/credit-risk-scorecard/reports/eda_summary.json'), WindowsPath('C:/Users/balla/Documents/credit-risk-scorecard/reports/feature_schema.json')]\n"
     ]
    }
   ],
   "source": [
    "print(\"data/raw:\", list((REPO/\"data\"/\"raw\").glob(\"*\")))\n",
    "print(\"splits:\",   list((REPO/\"splits\").glob(\"*\")))\n",
    "print(\"reports:\",  list((REPO/\"reports\").glob(\"*\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1152935-a61e-4833-b6dc-7734f970ea9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: C:\\Users\\balla\\Documents\\credit-risk-scorecard\\data\\raw\\german_credit.csv\n"
     ]
    }
   ],
   "source": [
    "import json, os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "import joblib\n",
    "\n",
    "\n",
    "REPO        = Path.cwd().parent                   \n",
    "DATA_PATH   = REPO / \"data\" / \"raw\" / \"german_credit.csv\"\n",
    "SPLITS_DIR  = REPO / \"splits\"\n",
    "REPORTS_DIR = REPO / \"reports\"\n",
    "MODELS_DIR  = REPO / \"models\"\n",
    "\n",
    "print(\"Using:\", DATA_PATH)\n",
    "\n",
    "# --- load data + splits + schema ---\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "train_idx = pd.read_csv(SPLITS_DIR / \"train_idx.csv\")[\"index\"].to_numpy()\n",
    "valid_idx = pd.read_csv(SPLITS_DIR / \"valid_idx.csv\")[\"index\"].to_numpy()\n",
    "test_idx  = pd.read_csv(SPLITS_DIR  / \"test_idx.csv\")[\"index\"].to_numpy()\n",
    "\n",
    "with open(REPORTS_DIR / \"feature_schema.json\") as f:\n",
    "    schema = json.load(f)\n",
    "num_cols, cat_cols = schema[\"numeric\"], schema[\"categorical\"]\n",
    "target = schema.get(\"target\", \"default\")\n",
    "\n",
    "X_train, y_train = df.loc[train_idx, num_cols + cat_cols], df.loc[train_idx, target]\n",
    "X_valid, y_valid = df.loc[valid_idx, num_cols + cat_cols], df.loc[valid_idx, target]\n",
    "X_test,  y_test  = df.loc[test_idx,  num_cols + cat_cols], df.loc[test_idx,  target]\n",
    "\n",
    "len(X_train), len(X_valid), len(X_test), y_train.mean()\n",
    "\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86202ac9-26c4-4fae-b9b3-cf11a410d985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trained + saved baseline'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "num_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "])\n",
    "pre = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols),\n",
    "])\n",
    "\n",
    "# model\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "pipe = Pipeline([(\"pre\", pre), (\"clf\", logreg)])\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(pipe, MODELS_DIR / \"baseline_logreg.pkl\")\n",
    "\"trained + saved baseline\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4b8d6a3-1852-4d9a-943c-ca07a5a56176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'roc_auc': 0.805595238095238,\n",
       "  'pr_auc': 0.6453582728321428,\n",
       "  'brier': 0.18685424120648478,\n",
       "  'ks': 0.4642857142857143,\n",
       "  'base_rate': 0.3},\n",
       " 0.6199655024289628,\n",
       " [[114, 26], [21, 39]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ks_stat(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return float(np.max(tpr - fpr))\n",
    "\n",
    "p_valid = pipe.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "metrics_valid = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_valid, p_valid)),\n",
    "    \"pr_auc\": float(average_precision_score(y_valid, p_valid)),\n",
    "    \"brier\": float(brier_score_loss(y_valid, p_valid)),\n",
    "    \"ks\": ks_stat(y_valid, p_valid),\n",
    "    \"base_rate\": float(y_valid.mean()),\n",
    "}\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_valid, p_valid)\n",
    "f1 = (2 * prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "best_i = int(np.nanargmax(f1))\n",
    "best_thr = float(thr[best_i])\n",
    "\n",
    "y_valid_hat = (p_valid >= best_thr).astype(int)\n",
    "cm_valid = confusion_matrix(y_valid, y_valid_hat).tolist()\n",
    "\n",
    "Path(\"reports\").mkdir(parents=True, exist_ok=True)\n",
    "prob_true, prob_pred = calibration_curve(y_valid, p_valid, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure()\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Observed frequency\")\n",
    "plt.title(\"Calibration â€” Baseline (validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/calibration_baseline_valid.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "with open(REPORTS_DIR / \"metrics_baseline_valid.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"metrics_valid\": metrics_valid,\n",
    "        \"best_threshold\": best_thr,\n",
    "        \"confusion_valid\": cm_valid\n",
    "    }, f, indent=2)\n",
    "\n",
    "metrics_valid, best_thr, cm_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37a5c885-dbfb-4320-ab62-0c9da9128a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'roc_auc': 0.7901190476190476,\n",
       "  'pr_auc': 0.6217050843038479,\n",
       "  'brier': 0.18775861644483072,\n",
       "  'ks': 0.48809523809523814,\n",
       "  'base_rate': 0.3,\n",
       "  'threshold_used': 0.6199655024289628},\n",
       " [[113, 27], [23, 37]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "metrics_test = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_test, p_test)),\n",
    "    \"pr_auc\": float(average_precision_score(y_test, p_test)),\n",
    "    \"brier\": float(brier_score_loss(y_test, p_test)),\n",
    "    \"ks\": ks_stat(y_test, p_test),\n",
    "    \"base_rate\": float(y_test.mean()),\n",
    "    \"threshold_used\": best_thr,\n",
    "}\n",
    "y_test_hat = (p_test >= best_thr).astype(int)\n",
    "cm_test = confusion_matrix(y_test, y_test_hat).tolist()\n",
    "\n",
    "with open(REPORTS_DIR / \"metrics_baseline_test.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"metrics_test\": metrics_test,\n",
    "        \"confusion_test\": cm_test\n",
    "    }, f, indent=2)\n",
    "\n",
    "metrics_test, cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b347ab0-4c51-46a0-b13f-69300b0c2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplace smoothing \n",
    "_EPS = 0.5\n",
    "\n",
    "def _woe_table(bin_series: pd.Series, y: pd.Series, eps=_EPS):\n",
    "    dfb = pd.DataFrame({\"bin\": bin_series, \"y\": y}).dropna()\n",
    "    g = dfb.groupby(\"bin\").y\n",
    "    good = (g.count() - g.sum()).astype(float)\n",
    "    bad  = g.sum().astype(float)\n",
    "    good_t, bad_t = good.sum(), bad.sum()\n",
    "    woe = np.log(((good + eps)/(good_t + eps)) / ((bad + eps)/(bad_t + eps)))\n",
    "    iv  = (((good + eps)/(good_t + eps)) - ((bad + eps)/(bad_t + eps))) * woe\n",
    "    tab = pd.DataFrame({\"good\": good, \"bad\": bad, \"woe\": woe, \"iv\": iv})\n",
    "    return tab, float(iv.sum())\n",
    "\n",
    "def fit_woe_numeric(x: pd.Series, y: pd.Series, q: int = 5):\n",
    "    # quantile bins; drop duplicate edges if needed\n",
    "    binned, edges = None, None\n",
    "    for k in range(q, 1, -1):  \n",
    "        try:\n",
    "            binned, edges = pd.qcut(x, q=k, duplicates=\"drop\", retbins=True)\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if binned is None:\n",
    "        # fallback: single bin â†’ zero WOE\n",
    "        return {\"type\": \"numeric\", \"edges\": [], \"woe_by_bin\": {}, \"iv\": 0.0}\n",
    "    tab, iv = _woe_table(binned, y)\n",
    "    # store WOE keyed by interval string\n",
    "    woe_by_bin = {str(idx): float(v) for idx, v in tab[\"woe\"].items()}\n",
    "    return {\"type\": \"numeric\", \"edges\": edges.tolist(), \"woe_by_bin\": woe_by_bin, \"iv\": iv}\n",
    "\n",
    "def fit_woe_categorical(x: pd.Series, y: pd.Series):\n",
    "    x = x.astype(\"object\")\n",
    "    tab, iv = _woe_table(x, y)\n",
    "    woe_by_cat = {str(idx): float(v) for idx, v in tab[\"woe\"].items()}\n",
    "    return {\"type\": \"categorical\", \"woe_by_cat\": woe_by_cat, \"iv\": iv}\n",
    "\n",
    "def apply_woe_numeric(x: pd.Series, edges, woe_by_bin):\n",
    "    if not edges:\n",
    "        return pd.Series(0.0, index=x.index)\n",
    "    binned = pd.cut(x, bins=np.array(edges), include_lowest=True)\n",
    "    s = binned.astype(str).map(woe_by_bin)\n",
    "    return s.fillna(0.0)\n",
    "\n",
    "def apply_woe_categorical(x: pd.Series, woe_by_cat):\n",
    "    s = x.astype(\"object\").map(woe_by_cat)\n",
    "    return s.fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfd1626d-d051-40e4-84fb-cea26e6b8263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2361938248.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = dfb.groupby(\"bin\").y\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2361938248.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = dfb.groupby(\"bin\").y\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2361938248.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = dfb.groupby(\"bin\").y\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2361938248.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = dfb.groupby(\"bin\").y\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2361938248.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = dfb.groupby(\"bin\").y\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2361938248.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = dfb.groupby(\"bin\").y\n",
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2361938248.py:6: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  g = dfb.groupby(\"bin\").y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     woe_duration  woe_credit_amount  woe_installment_commitment  \\\n",
       " 381     -0.163576          -0.339293                    0.001584   \n",
       " 580     -0.163576          -0.006274                    0.001584   \n",
       " 88      -0.163576           0.021505                   -0.165605   \n",
       " \n",
       "      woe_residence_since   woe_age  woe_existing_credits  woe_num_dependents  \\\n",
       " 381            -0.012181  0.164233             -0.001659                 0.0   \n",
       " 580            -0.012181  0.067907             -0.001659                 0.0   \n",
       " 88             -0.012181  0.067907             -0.001659                 0.0   \n",
       " \n",
       "      woe_checking_status  woe_credit_history  woe_purpose  woe_savings_status  \\\n",
       " 381            -0.399757           -0.087562     0.589371           -0.293184   \n",
       " 580            -0.399757            0.743176    -0.187835           -0.293184   \n",
       " 88             -0.714574           -0.087562    -0.187835           -0.163262   \n",
       " \n",
       "      woe_employment  woe_personal_status  woe_other_parties  \\\n",
       " 381       -0.845714            -0.256179           0.001584   \n",
       " 580        0.236301             0.156298           0.606539   \n",
       " 88         0.560648             0.156298           0.001584   \n",
       " \n",
       "      woe_property_magnitude  woe_other_payment_plans  woe_housing   woe_job  \\\n",
       " 381               -0.479989                 0.188099    -0.343085 -0.127461   \n",
       " 580                0.451076                -0.726525     0.219461  0.047900   \n",
       " 88                -0.069636                 0.188099     0.219461 -0.127461   \n",
       " \n",
       "      woe_own_telephone  woe_foreign_worker  \n",
       " 381           0.172210           -0.048181  \n",
       " 580          -0.112603           -0.048181  \n",
       " 88            0.172210           -0.048181  ,\n",
       "      woe_duration  woe_credit_amount  woe_installment_commitment  \\\n",
       " 742     -0.163576           0.241909                   -0.165605   \n",
       " 673      0.447687           0.021505                    0.222956   \n",
       " 544      0.447687          -0.006274                   -0.165605   \n",
       " \n",
       "      woe_residence_since   woe_age  woe_existing_credits  woe_num_dependents  \\\n",
       " 742            -0.012181  0.164233             -0.001659                 0.0   \n",
       " 673             0.012726 -0.393729             -0.001659                 0.0   \n",
       " 544            -0.012181  0.079492             -0.001659                 0.0   \n",
       " \n",
       "      woe_checking_status  woe_credit_history  woe_purpose  woe_savings_status  \\\n",
       " 742             1.027271           -0.087562     0.351744            0.708154   \n",
       " 673             1.027271            0.743176    -0.187835            0.732472   \n",
       " 544             1.027271            0.743176    -0.187835           -0.293184   \n",
       " \n",
       "      woe_employment  woe_personal_status  woe_other_parties  \\\n",
       " 742        0.236301             0.156298           0.001584   \n",
       " 673        0.039705             0.089747           0.001584   \n",
       " 544        0.236301             0.156298           0.001584   \n",
       " \n",
       "      woe_property_magnitude  woe_other_payment_plans  woe_housing  woe_job  \\\n",
       " 742               -0.064013                 0.188099     0.219461   0.0479   \n",
       " 673               -0.069636                 0.188099     0.219461   0.0479   \n",
       " 544                0.451076                 0.188099     0.219461   0.0096   \n",
       " \n",
       "      woe_own_telephone  woe_foreign_worker  \n",
       " 742           0.172210           -0.048181  \n",
       " 673          -0.112603           -0.048181  \n",
       " 544          -0.112603           -0.048181  )"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "BIN_PATH = REPORTS_DIR / \"binning.json\"\n",
    "IV_PATH  = REPORTS_DIR / \"iv_table.csv\"\n",
    "\n",
    "woe_cfg = {}\n",
    "iv_rows = []\n",
    "\n",
    "# Fit on TRAIN only\n",
    "for col in num_cols:\n",
    "    cfg = fit_woe_numeric(X_train[col], y_train, q=5)\n",
    "    woe_cfg[col] = cfg\n",
    "    iv_rows.append({\"feature\": col, \"type\": \"num\", \"iv\": cfg[\"iv\"]})\n",
    "\n",
    "for col in cat_cols:\n",
    "    cfg = fit_woe_categorical(X_train[col], y_train)\n",
    "    woe_cfg[col] = cfg\n",
    "    iv_rows.append({\"feature\": col, \"type\": \"cat\", \"iv\": cfg[\"iv\"]})\n",
    "\n",
    "# Save binning + IV table\n",
    "with open(BIN_PATH, \"w\") as f:\n",
    "    json.dump(woe_cfg, f, indent=2)\n",
    "pd.DataFrame(iv_rows).sort_values(\"iv\", ascending=False).to_csv(IV_PATH, index=False)\n",
    "\n",
    "# Transform â†’ WOE space\n",
    "def to_woe_frame(df_part: pd.DataFrame):\n",
    "    cols = []\n",
    "    for col in num_cols:\n",
    "        cfg = woe_cfg[col]\n",
    "        s = apply_woe_numeric(df_part[col], cfg[\"edges\"], cfg[\"woe_by_bin\"])\n",
    "        cols.append(s.rename(f\"woe_{col}\"))\n",
    "    for col in cat_cols:\n",
    "        cfg = woe_cfg[col]\n",
    "        s = apply_woe_categorical(df_part[col], cfg[\"woe_by_cat\"])\n",
    "        cols.append(s.rename(f\"woe_{col}\"))\n",
    "    return pd.concat(cols, axis=1)\n",
    "\n",
    "Xtr_w, Xva_w, Xte_w = to_woe_frame(X_train), to_woe_frame(X_valid), to_woe_frame(X_test)\n",
    "Xtr_w.head(3), Xva_w.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3ee7bc5a-baea-4ae6-b04f-bdf96f16fbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'roc_auc': 0.8026190476190477,\n",
       "  'pr_auc': 0.6447269935240115,\n",
       "  'brier': 0.18317461409080202,\n",
       "  'ks': 0.4904761904761905,\n",
       "  'base_rate': 0.3},\n",
       " 0.49179808894079324,\n",
       " {'roc_auc': 0.7899999999999999,\n",
       "  'pr_auc': 0.5903092788969597,\n",
       "  'brier': 0.18531904333479268,\n",
       "  'ks': 0.519047619047619,\n",
       "  'base_rate': 0.3,\n",
       "  'threshold_used': 0.49179808894079324})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scard = LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\")\n",
    "scard.fit(Xtr_w, y_train)\n",
    "\n",
    "# VALID\n",
    "p_valid_sc = scard.predict_proba(Xva_w)[:, 1]\n",
    "def ks_stat(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return float((tpr - fpr).max())\n",
    "\n",
    "metrics_valid_sc = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_valid, p_valid_sc)),\n",
    "    \"pr_auc\": float(average_precision_score(y_valid, p_valid_sc)),\n",
    "    \"brier\": float(brier_score_loss(y_valid, p_valid_sc)),\n",
    "    \"ks\": ks_stat(y_valid, p_valid_sc),\n",
    "    \"base_rate\": float(y_valid.mean()),\n",
    "}\n",
    "\n",
    "# threshold by max F1 on VALID\n",
    "prec, rec, thr = precision_recall_curve(y_valid, p_valid_sc)\n",
    "f1 = (2 * prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-12)\n",
    "best_i_sc = int(np.nanargmax(f1))\n",
    "best_thr_sc = float(thr[best_i_sc])\n",
    "\n",
    "# calibration plot (VALID)\n",
    "prob_true, prob_pred = calibration_curve(y_valid, p_valid_sc, n_bins=10, strategy=\"quantile\")\n",
    "plt.figure()\n",
    "plt.plot([0,1],[0,1],\"--\")\n",
    "plt.plot(prob_pred, prob_true, marker=\"o\")\n",
    "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Observed frequency\")\n",
    "plt.title(\"Calibration â€” Scorecard (validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / \"calibration_scorecard_valid.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# TEST with fixed threshold\n",
    "p_test_sc = scard.predict_proba(Xte_w)[:, 1]\n",
    "metrics_test_sc = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_test, p_test_sc)),\n",
    "    \"pr_auc\": float(average_precision_score(y_test, p_test_sc)),\n",
    "    \"brier\": float(brier_score_loss(y_test, p_test_sc)),\n",
    "    \"ks\": ks_stat(y_test, p_test_sc),\n",
    "    \"base_rate\": float(y_test.mean()),\n",
    "    \"threshold_used\": best_thr_sc,\n",
    "}\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_valid_sc = confusion_matrix(y_valid, (p_valid_sc >= best_thr_sc)).tolist()\n",
    "cm_test_sc  = confusion_matrix(y_test,  (p_test_sc  >= best_thr_sc)).tolist()\n",
    "\n",
    "# Save artifacts\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "import joblib, json\n",
    "joblib.dump(scard, MODELS_DIR / \"scorecard_logreg.pkl\")\n",
    "with open(REPORTS_DIR / \"metrics_scorecard_valid.json\",\"w\") as f:\n",
    "    json.dump({\"metrics_valid\": metrics_valid_sc,\n",
    "               \"best_threshold\": best_thr_sc,\n",
    "               \"confusion_valid\": cm_valid_sc}, f, indent=2)\n",
    "with open(REPORTS_DIR / \"metrics_scorecard_test.json\",\"w\") as f:\n",
    "    json.dump({\"metrics_test\": metrics_test_sc,\n",
    "               \"confusion_test\": cm_test_sc}, f, indent=2)\n",
    "\n",
    "metrics_valid_sc, best_thr_sc, metrics_test_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f478995c-a851-4471-a097-90ebaeef7e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'valid': {'baseline': {'roc_auc': 0.805595238095238,\n",
       "   'pr_auc': 0.6453582728321428,\n",
       "   'brier': 0.18685424120648478,\n",
       "   'ks': 0.4642857142857143,\n",
       "   'base_rate': 0.3},\n",
       "  'scorecard': {'roc_auc': 0.8026190476190477,\n",
       "   'pr_auc': 0.6447269935240115,\n",
       "   'brier': 0.18317461409080202,\n",
       "   'ks': 0.4904761904761905,\n",
       "   'base_rate': 0.3}},\n",
       " 'test': {'baseline': {'roc_auc': 0.7901190476190476,\n",
       "   'pr_auc': 0.6217050843038479,\n",
       "   'brier': 0.18775861644483072,\n",
       "   'ks': 0.48809523809523814,\n",
       "   'base_rate': 0.3,\n",
       "   'threshold_used': 0.6199655024289628},\n",
       "  'scorecard': {'roc_auc': 0.7899999999999999,\n",
       "   'pr_auc': 0.5903092788969597,\n",
       "   'brier': 0.18531904333479268,\n",
       "   'ks': 0.519047619047619,\n",
       "   'base_rate': 0.3,\n",
       "   'threshold_used': 0.49179808894079324}}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(REPORTS_DIR / \"metrics_baseline_valid.json\") as f: base_v = json.load(f)[\"metrics_valid\"]\n",
    "with open(REPORTS_DIR / \"metrics_baseline_test.json\")  as f: base_t = json.load(f)[\"metrics_test\"]\n",
    "with open(REPORTS_DIR / \"metrics_scorecard_valid.json\") as f: sc_v = json.load(f)[\"metrics_valid\"]\n",
    "with open(REPORTS_DIR / \"metrics_scorecard_test.json\")  as f: sc_t = json.load(f)[\"metrics_test\"]\n",
    "{\"valid\": {\"baseline\": base_v, \"scorecard\": sc_v},\n",
    " \"test\":  {\"baseline\": base_t, \"scorecard\": sc_t}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d55ae85e-3783-4224-bad6-11c95c2f54d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'roc_auc': 0.8227380952380953,\n",
       "  'pr_auc': 0.646482977471909,\n",
       "  'brier': 0.14843071736896365,\n",
       "  'ks': 0.4904761904761905,\n",
       "  'base_rate': 0.3},\n",
       " {'roc_auc': 0.7845238095238095,\n",
       "  'pr_auc': 0.5727944470389871,\n",
       "  'brier': 0.16373407110788954,\n",
       "  'ks': 0.5023809523809524,\n",
       "  'base_rate': 0.3})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "import numpy as np, json\n",
    "\n",
    "# Use the scorecard raw scores you already have:\n",
    "# p_valid_sc, p_test_sc from Cell G\n",
    "\n",
    "iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "iso.fit(p_valid_sc, y_valid.astype(float))\n",
    "\n",
    "p_valid_sc_cal = iso.transform(p_valid_sc)\n",
    "p_test_sc_cal  = iso.transform(p_test_sc)\n",
    "\n",
    "def ks_stat(y_true, y_score):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    return float((tpr - fpr).max())\n",
    "\n",
    "cal_valid = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_valid, p_valid_sc_cal)),   # (won't change much)\n",
    "    \"pr_auc\": float(average_precision_score(y_valid, p_valid_sc_cal)),\n",
    "    \"brier\":  float(brier_score_loss(y_valid, p_valid_sc_cal)), # should â†“ or hold\n",
    "    \"ks\":     ks_stat(y_valid, p_valid_sc_cal),\n",
    "    \"base_rate\": float(y_valid.mean()),\n",
    "}\n",
    "\n",
    "cal_test = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_test, p_test_sc_cal)),\n",
    "    \"pr_auc\": float(average_precision_score(y_test, p_test_sc_cal)),\n",
    "    \"brier\":  float(brier_score_loss(y_test, p_test_sc_cal)),\n",
    "    \"ks\":     ks_stat(y_test, p_test_sc_cal),\n",
    "    \"base_rate\": float(y_test.mean()),\n",
    "}\n",
    "\n",
    "with open(REPORTS_DIR / \"metrics_scorecard_valid_calibrated.json\",\"w\") as f:\n",
    "    json.dump(cal_valid, f, indent=2)\n",
    "with open(REPORTS_DIR / \"metrics_scorecard_test_calibrated.json\",\"w\") as f:\n",
    "    json.dump(cal_test, f, indent=2)\n",
    "\n",
    "cal_valid, cal_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c589d624-3ef5-496a-ad26-c01d40a467d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.075, np.int64(92))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose threshold by simple expected-cost (FN = 5 x FP)\n",
    "thr_grid = np.linspace(0.05, 0.95, 181)\n",
    "best_thr_cost = None\n",
    "best_cost = float(\"inf\")\n",
    "\n",
    "for t in thr_grid:\n",
    "    yhat = (p_valid_sc_cal >= t).astype(int)\n",
    "    # confusion entries: tn, fp, fn, tp\n",
    "    tn, fp, fn, tp = confusion_matrix(y_valid, yhat).ravel()\n",
    "    cost = 5*fn + 1*fp\n",
    "    if cost < best_cost:\n",
    "        best_cost, best_thr_cost = cost, float(t)\n",
    "\n",
    "best_thr_cost, best_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dbe73b91-1962-407f-877e-6a13c7ca8397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balla\\AppData\\Local\\Temp\\ipykernel_26116\\2801505644.py:15: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  summary = bucket_df.groupby(\"bucket\").agg(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bucket</th>\n",
       "      <th>n</th>\n",
       "      <th>default_rate</th>\n",
       "      <th>pd_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Very Low</td>\n",
       "      <td>39</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.027230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Low</td>\n",
       "      <td>35</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.073171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Medium</td>\n",
       "      <td>22</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High</td>\n",
       "      <td>104</td>\n",
       "      <td>0.490385</td>\n",
       "      <td>0.496315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bucket    n  default_rate   pd_mean\n",
       "0  Very Low   39      0.076923  0.027230\n",
       "1       Low   35      0.085714  0.073171\n",
       "2    Medium   22      0.136364  0.187500\n",
       "3      High  104      0.490385  0.496315"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# bucket edges (tune if you like)\n",
    "edges = np.array([0.0, 0.05, 0.10, 0.20, 1.0])\n",
    "labels = [\"Very Low\", \"Low\", \"Medium\", \"High\"]\n",
    "\n",
    "# assign buckets on TEST (holdout)\n",
    "buckets = pd.cut(p_test_sc_cal, bins=edges, labels=labels, include_lowest=True)\n",
    "\n",
    "bucket_df = pd.DataFrame({\n",
    "    \"bucket\": buckets,\n",
    "    \"pd\": p_test_sc_cal,\n",
    "    \"y\": y_test.values\n",
    "})\n",
    "summary = bucket_df.groupby(\"bucket\").agg(\n",
    "    n=(\"y\",\"size\"),\n",
    "    default_rate=(\"y\",\"mean\"),\n",
    "    pd_mean=(\"pd\",\"mean\")\n",
    ").reset_index()\n",
    "\n",
    "summary.to_csv(REPORTS_DIR / \"risk_buckets_test.csv\", index=False)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a91b37e5-abd4-4da6-ab16-c730c189e82c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>type</th>\n",
       "      <th>iv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>checking_status</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.534943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>credit_history</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.258963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>savings_status</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.246377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>employment</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.160335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duration</td>\n",
       "      <td>num</td>\n",
       "      <td>0.140566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>other_payment_plans</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.131480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>purpose</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.121404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>housing</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.117337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>property_magnitude</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.093637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>foreign_worker</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.086427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature type        iv\n",
       "0      checking_status  cat  0.534943\n",
       "1       credit_history  cat  0.258963\n",
       "2       savings_status  cat  0.246377\n",
       "3           employment  cat  0.160335\n",
       "4             duration  num  0.140566\n",
       "5  other_payment_plans  cat  0.131480\n",
       "6              purpose  cat  0.121404\n",
       "7              housing  cat  0.117337\n",
       "8   property_magnitude  cat  0.093637\n",
       "9       foreign_worker  cat  0.086427"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(REPORTS_DIR / \"iv_table.csv\").sort_values(\"iv\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383e324d-4394-4cd2-9e54-91b0e559cf6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
